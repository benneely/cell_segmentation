import numpy as np
import tensorflow as tf
import keras.backend as K
import keras.layers as KL
import keras.models as KM
from mrcnn_bd import mrcnn_utils as utils


def model(
        image_shape,
        num_classes,
        batch_size,
        mode='training',
        use_mini_mask=False,
        mini_mask_shape=(56, 56),
        rpn_anchor_stride=1,
        # Ratios of anchors at each cell (width/height)
        # A value of 1 represents a square anchor, and 0.5 is a wide anchor
        rpn_anchor_ratios=(0.5, 1, 2),
        # ROIs kept after non-maximum supression (training and inference)
        post_nms_rois_training = 2000,
        post_nms_rois_inference = 1000,
        # Non-max suppression threshold to filter RPN proposals.
        # You can increase this during training to generate more propsals.
        rpn_nms_threshold=0.7,
        images_per_gpu=2,
        # Bounding box refinement standard deviation for RPN and final detections.
        rpn_bbox_std_dev=np.array([0.1, 0.1, 0.2, 0.2]),
        bbox_std_dev=np.array([0.1, 0.1, 0.2, 0.2]),
        # Use RPN ROIs or externally generated ROIs for training
        # Keep this True for most situations. Set to False if you want to train
        # the head branches on ROI generated by code rather than the ROIs from
        # the RPN. For example, to debug the classifier head without having to
        # train the RPN.
        use_rpn_rois=True,
        # Shape of output mask
        # To change this you also need to change the neural network mask branch
        mask_shape=(28, 28),
        # Percent of positive ROIs used to train classifier/mask heads
        roi_positive_ratio=0.33,
        # Number of ROIs per image to feed to classifier/mask heads
        # The Mask RCNN paper uses 512 but often the RPN doesn't generate
        # enough positive proposals to fill this and keep a positive:negative
        # ratio of 1:3. You can increase the number of proposals by adjusting
        # the RPN NMS threshold.
        train_rois_per_image=200,
        # Pooled ROIs
        pool_size=7,
        mask_pool_size=14,
        train_bn=False,
        backbone_strides=(4, 8, 16, 32, 64),
        rpn_anchor_scales=(32, 64, 128, 256, 512)
):
    IMAGE_META_SIZE = 1 + 3 + 3 + 4 + 1 + num_classes
    assert mode in ['training', 'inference']
    config = {
        'TRAIN_ROIS_PER_IMAGE': train_rois_per_image,
        'ROI_POSITIVE_RATIO': roi_positive_ratio,
        'BBOX_STD_DEV': bbox_std_dev,
        'USE_MINI_MASK': use_mini_mask,
        'MASK_SHAPE': mask_shape,
        'IMAGES_PER_GPU': images_per_gpu
    }
    # Image size must be dividable by 2 multiple times
    h, w = image_shape[:2]
    if h / 2 ** 6 != int(h / 2 ** 6) or w / 2 ** 6 != int(w / 2 ** 6):
        raise Exception(
            "Image size must be dividable by 2 at least 6 times to avoid fractions when downscaling and upscaling."
            "For example, use 256, 320, 384, 448, 512, ... etc. "
        )
    # Inputs
    input_image = KL.Input(
        shape=[None, None, 3], name="input_image"
    )
    input_image_meta = KL.Input(
        shape=[IMAGE_META_SIZE],
        name="input_image_meta"
    )
    if mode == "training":
        # RPN GT
        input_rpn_match = KL.Input(
            shape=[None, 1],
            name="input_rpn_match",
            dtype=tf.int32
        )
        input_rpn_bbox = KL.Input(
            shape=[None, 4],
            name="input_rpn_bbox",
            dtype=tf.float32
        )
        ##############################################################################################################
        # Detection GT (class IDs, bounding boxes, and masks)
        # 1. GT Class IDs (zero padded)
        ##############################################################################################################
        input_gt_class_ids = KL.Input(
            shape=[None],
            name="input_gt_class_ids",
            dtype=tf.int32
        )
        ##############################################################################################################
        # 2. GT Boxes in pixels (zero padded)
        # [batch, MAX_GT_INSTANCES, (y1, x1, y2, x2)] in image coordinates
        ##############################################################################################################
        input_gt_boxes = KL.Input(
            shape=[None, 4],
            name="input_gt_boxes",
            dtype=tf.float32
        )
        # Normalize coordinates
        gt_boxes = KL.Lambda(
            lambda x: utils.norm_boxes_graph(
                x,
                K.shape(input_image)[1:3]
            )
        )(input_gt_boxes)
        ##############################################################################################################
        # 3. GT Masks (zero padded)
        # [batch, height, width, MAX_GT_INSTANCES]
        ##############################################################################################################
        if use_mini_mask:
            input_gt_masks = KL.Input(
                shape=[mini_mask_shape[0], mini_mask_shape[1], None],
                name="input_gt_masks",
                dtype=bool
            )
        else:
            input_gt_masks = KL.Input(
                shape=[image_shape[0], image_shape[1], None],
                name="input_gt_masks",
                dtype=bool
            )
    elif mode == "inference":
        # Anchors in normalized coordinates
        input_anchors = KL.Input(
            shape=[None, 4],
            name="input_anchors"
        )
    ##############################################################################################################
    # 4. Build the shared convolutional layers.
    # Bottom-up Layers
    # Returns a list of the last layers of each stage, 5 in total.
    # Don't create the thead (stage 5), so we pick the 4th item in the list.
    ##############################################################################################################
    _, C2, C3, C4, C5 = utils.resnet_graph(
        input_image,
        "resnet101",
        stage5=True,
        train_bn=False
    )
    # Top-down Layers
    # TODO: add assert to varify feature map sizes match what's in config
    P5 = KL.Conv2D(
        256,
        (1, 1),
        name='fpn_c5p5'
    )(C5)
    P4 = KL.Add(
        name="fpn_p4add"
    )([KL.UpSampling2D(size=(2, 2), name="fpn_p5upsampled")(P5), KL.Conv2D(256, (1, 1), name='fpn_c4p4')(C4)])
    P3 = KL.Add(
        name="fpn_p3add"
    )([KL.UpSampling2D(size=(2, 2), name="fpn_p4upsampled")(P4), KL.Conv2D(256, (1, 1), name='fpn_c3p3')(C3)])
    P2 = KL.Add(
        name="fpn_p2add"
    )([KL.UpSampling2D(size=(2, 2), name="fpn_p3upsampled")(P3), KL.Conv2D(256, (1, 1), name='fpn_c2p2')(C2)])
    # Attach 3x3 conv to all P layers to get the final feature maps.
    P2 = KL.Conv2D(256, (3, 3), padding="SAME", name="fpn_p2")(P2)
    P3 = KL.Conv2D(256, (3, 3), padding="SAME", name="fpn_p3")(P3)
    P4 = KL.Conv2D(256, (3, 3), padding="SAME", name="fpn_p4")(P4)
    P5 = KL.Conv2D(256, (3, 3), padding="SAME", name="fpn_p5")(P5)
    # P6 is used for the 5th anchor scale in RPN. Generated by
    # subsampling from P5 with stride of 2.
    P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name="fpn_p6")(P5)
    # Note that P6 is used in RPN, but not in the classifier heads.
    rpn_feature_maps = [P2, P3, P4, P5, P6]
    mrcnn_feature_maps = [P2, P3, P4, P5]
    ##############################################################################################################
    # 5. Anchors
    ##############################################################################################################
    if mode == "training":
        anchors = utils.get_anchors(
            backbone_strides,
            image_shape,
            rpn_anchor_scales,
            rpn_anchor_ratios,
            rpn_anchor_stride
        )
        # Duplicate across the batch dimension because Keras requires it
        # TODO: can this be optimized to avoid duplicating the anchors?
        anchors = np.broadcast_to(anchors, (batch_size,) + anchors.shape)
        # A hack to get around Keras's bad support for constants
        anchors = KL.Lambda(lambda x: tf.Variable(anchors), name="anchors")(input_image)
    else:
        anchors = input_anchors
    ##############################################################################################################
    # 6. RPN Model
    ##############################################################################################################
    rpn = utils.build_rpn_model(
        rpn_anchor_stride,
        len(rpn_anchor_ratios),
        256
    )
    ##############################################################################################################
    # 7. Loop through pyramid layers
    ##############################################################################################################
    layer_outputs = []  # list of lists
    for p in rpn_feature_maps:
        layer_outputs.append(rpn([p]))
    # Concatenate layer outputs
    # Convert from list of lists of level outputs to list of lists
    # of outputs across levels.
    # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]
    output_names = ["rpn_class_logits", "rpn_class", "rpn_bbox"]
    outputs = list(zip(*layer_outputs))
    outputs = [KL.Concatenate(axis=1, name=n)(list(o))
               for o, n in zip(outputs, output_names)]

    rpn_class_logits, rpn_class, rpn_bbox = outputs
    ##############################################################################################################
    # 8. Generate proposals
    # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates
    # and zero padded.
    ##############################################################################################################
    proposal_count = post_nms_rois_training if mode == "training" else post_nms_rois_inference
    rpn_rois = utils.ProposalLayer(
        proposal_count=proposal_count,
        nms_threshold=rpn_nms_threshold,
        name="ROI",
        images_per_gpu=images_per_gpu,
        rpn_bbox_std_dev = rpn_bbox_std_dev
    )([rpn_class, rpn_bbox, anchors])

    if mode == "training":
        # Class ID mask to mark class IDs supported by the data set the image
        # came from.
        active_class_ids = KL.Lambda(
            lambda x: utils.parse_image_meta_graph(x)["active_class_ids"]
        )(input_image_meta)

        if not use_rpn_rois:
            # Ignore predicted ROIs and use ROIs provided as an input.
            input_rois = KL.Input(
                shape=[post_nms_rois_training, 4],
                name="input_roi",
                dtype=np.int32
            )
            # Normalize coordinates
            target_rois = KL.Lambda(lambda x: utils.norm_boxes_graph(
                x, K.shape(input_image)[1:3]))(input_rois)
        else:
            target_rois = rpn_rois

        # Generate detection targets
        # Subsamples proposals and generates target outputs for training
        # Note that proposal class IDs, gt_boxes, and gt_masks are zero
        # padded. Equally, returned rois and targets are zero padded.
        rois, target_class_ids, target_bbox, target_mask = \
            utils.DetectionTargetLayer(
                config,
                name="proposal_targets"
            )([target_rois, input_gt_class_ids, gt_boxes, input_gt_masks])

        # Network Heads
        # TODO: verify that this handles zero padded ROIs
        mrcnn_class_logits, mrcnn_class, mrcnn_bbox = utils.fpn_classifier_graph(
                rois,
                mrcnn_feature_maps,
                input_image_meta,
                pool_size,
                num_classes,
                train_bn=train_bn
            )
        mrcnn_mask = utils.build_fpn_mask_graph(
            rois,
            mrcnn_feature_maps,
            input_image_meta,
            mask_pool_size,
            num_classes,
            train_bn=train_bn
        )
        # TODO: clean up (use tf.identify if necessary)
        output_rois = KL.Lambda(lambda x: x * 1, name="output_rois")(rois)
        # Losses
        rpn_class_loss = KL.Lambda(
            lambda x: utils.rpn_class_loss_graph(*x),
            name="rpn_class_loss"
        )([input_rpn_match, rpn_class_logits])
        rpn_bbox_loss = KL.Lambda(
            lambda x: utils.rpn_bbox_loss_graph(config, *x),
            name="rpn_bbox_loss"
        )([input_rpn_bbox, input_rpn_match, rpn_bbox])
        class_loss = KL.Lambda(
            lambda x: utils.mrcnn_class_loss_graph(*x),
            name="mrcnn_class_loss"
        )([target_class_ids, mrcnn_class_logits, active_class_ids])
        bbox_loss = KL.Lambda(
            lambda x: utils.mrcnn_bbox_loss_graph(*x),
            name="mrcnn_bbox_loss"
        )([target_bbox, target_class_ids, mrcnn_bbox])
        mask_loss = KL.Lambda(
            lambda x: utils.mrcnn_mask_loss_graph(*x),
            name="mrcnn_mask_loss"
        )([target_mask, target_class_ids, mrcnn_mask])
        # Model
        inputs = [
            input_image,
            input_image_meta,
            input_rpn_match,
            input_rpn_bbox,
            input_gt_class_ids,
            input_gt_boxes,
            input_gt_masks
        ]
        if not use_rpn_rois:
            inputs.append(input_rois)
        outputs = [
            rpn_class_logits,
            rpn_class,
            rpn_bbox,
            mrcnn_class_logits,
            mrcnn_class,
            mrcnn_bbox,
            mrcnn_mask,
            rpn_rois,
            output_rois,
            rpn_class_loss,
            rpn_bbox_loss,
            class_loss,
            bbox_loss,
            mask_loss
        ]
        model = KM.Model(
            inputs,
            outputs,
            name='mask_rcnn'
        )
    else:
        # Network Heads
        # Proposal classifier and BBox regressor heads
        mrcnn_class_logits, mrcnn_class, mrcnn_bbox = utils.fpn_classifier_graph(
            rpn_rois,
            mrcnn_feature_maps,
            input_image_meta,
            pool_size,
            num_classes,
            train_bn=train_bn
        )
        # Detections
        # output is [batch, num_detections, (y1, x1, y2, x2, class_id, score)] in
        # normalized coordinates
        detections = utils.DetectionLayer(
            config,
            name="mrcnn_detection"
        )([rpn_rois, mrcnn_class, mrcnn_bbox, input_image_meta])

        # Create masks for detections
        detection_boxes = KL.Lambda(lambda x: x[..., :4])(detections)
        mrcnn_mask = utils.build_fpn_mask_graph(
            detection_boxes,
            mrcnn_feature_maps,
            input_image_meta,
            mask_pool_size,
            config.NUM_CLASSES,
            train_bn=config.TRAIN_BN
        )
        model = KM.Model(
            [input_image, input_image_meta, input_anchors],
            [detections, mrcnn_class, mrcnn_bbox, mrcnn_mask, rpn_rois, rpn_class, rpn_bbox],
            name='mask_rcnn'
        )
    # Add multi-GPU support.
    # if config.GPU_COUNT > 1:
    #     from mrcnn.parallel_model import ParallelModel
    #     model = ParallelModel(model, config.GPU_COUNT)

    return model